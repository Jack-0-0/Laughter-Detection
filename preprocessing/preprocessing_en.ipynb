{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "from tqdm.notebook import tqdm\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set paths\n",
    "load_labels_path = '/laughter/DUEL/en/MS_word_laughter_timings/data/alignments'\n",
    "save_labels_path = '/laughter/DUEL/en/labels'\n",
    "load_audio_path = '/laughter/DUEL/en/switchboard1/swb1'\n",
    "save_spectros_path = '/laughter/DUEL/en/spectros'\n",
    "save_dataset_path = '/laughter/DUEL/datasets/en'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create spectrograms and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_text_give_save_path(text_file, save_labels_path):\n",
    "    \"\"\"Creates a filtered dataframe and accompanying save path.\n",
    "\n",
    "    # Arguments\n",
    "        text_file: .text file.\n",
    "\n",
    "    # Returns\n",
    "        df4: dataframe with rows labelled with laughter.\n",
    "        save_path: save path for df4.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(text_file, sep=\"\\t\", names=[\"id\", \"a\", \"start\", \"end\", \"x\", \"label1\", \"label2\"])\n",
    "    df1 = df[df['label1'].str.contains(\"laugh\")]\n",
    "    df2 = df[df['label2'].str.contains(\"laugh\")] \n",
    "    df3 = pd.concat([df1,df2]).drop_duplicates().reset_index(drop=True)\n",
    "    df4 = df3.drop(columns=['a', 'x'])\n",
    "    save_file = os.path.basename(text_file).split('-')[0] + '_Laugh.txt'\n",
    "    save_path = save_labels_path + '/' + save_file\n",
    "    return df4, save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_AB(a_path, save_labels_path, b_laughter_paths):\n",
    "    \"\"\"Creates combined dataframe of A and B labels.\n",
    "    \n",
    "    # Arguments\n",
    "        a_path: filepath to A label file.\n",
    "    \n",
    "    # Returns\n",
    "        ab_df: dataframe of combined A and B labels.\n",
    "        ab_save: save path.\n",
    "    \"\"\"\n",
    "    a_id = os.path.basename(a).split('A')[0]\n",
    "    b = [x for x in b_laughter_paths if a_id in x][0]\n",
    "    a_df = pd.read_csv(a, sep=\"\\t\")\n",
    "    b_df = pd.read_csv(b, sep=\"\\t\")\n",
    "    ab_df = pd.concat([a_df, b_df]).reset_index(drop=True).drop(columns=['Unnamed: 0'])\n",
    "    save_file = os.path.basename(a_path).split('A')[0] + '_Comb_Laugh.txt'\n",
    "    ab_save = save_labels_path + '/' + save_file\n",
    "    return ab_df, ab_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spectrogram(load_path, save_path, start, stop, y, sr, mels):\n",
    "    \"\"\"Creates a mel spectrogram and accompanying save path.\n",
    "\n",
    "    # Arguments\n",
    "        load_path: wav audio filepath.\n",
    "        save_path: directory to save spectrograms.\n",
    "        start: start time in seconds.\n",
    "        stop: stop time in seconds.\n",
    "        y: audio time series.\n",
    "        sr: sample rate.\n",
    "        \n",
    "    # Returns\n",
    "        S: mel spectrogram array with dimensions (n_mels, t).\n",
    "        S_save_path: path to save spetrogram.\n",
    "    \"\"\"\n",
    "    S = librosa.feature.melspectrogram(y=y[sr * start:(sr * stop)],\n",
    "                                       sr=sr, n_mels=mels, fmax=sr / 2)\n",
    "    c_id = os.path.basename(load_path).split('.')[0]\n",
    "    if not os.path.isdir(save_path + '/' + c_id):\n",
    "        os.makedirs(save_path + '/' + c_id)\n",
    "    S_save_path = save_path + '/' + c_id + '/' + c_id + '_' + str(start) + \\\n",
    "                  'to' + str(stop) + '_spectro'\n",
    "    return S, S_save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spectrogram settings\n",
    "timesteps= 259\n",
    "window_size = 6\n",
    "timesteps_per_second = timesteps / window_size\n",
    "slide = 6\n",
    "mels = 64\n",
    "sample_rate = 22_050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_files = [os.path.join(root, name)\n",
    "              for root, dirs, files in os.walk(load_labels_path)\n",
    "              for name in files\n",
    "              if name.endswith((\".text\"))]\n",
    "\n",
    "# filter for laughter and save label files\n",
    "for f in tqdm(label_files, desc='labels'):\n",
    "    df, save_path = filter_text_give_save_path(f, save_labels_path)\n",
    "    df.to_csv(save_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laughter_paths = [os.path.join(root, name)\n",
    "            for root, dirs, files in os.walk(save_labels_path)\n",
    "            for name in files\n",
    "            if name.endswith((\".txt\"))]\n",
    "\n",
    "a_laughter_paths = [os.path.join(root, name)\n",
    "            for root, dirs, files in os.walk(save_labels_path)\n",
    "            for name in files\n",
    "            if name.endswith((\"A_Laugh.txt\"))]\n",
    "\n",
    "b_laughter_paths = [os.path.join(root, name)\n",
    "            for root, dirs, files in os.walk(save_labels_path)\n",
    "            for name in files\n",
    "            if name.endswith((\"B_Laugh.txt\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine A and B laughter files\n",
    "for a in tqdm(a_laughter_paths, desc='combing labels'):\n",
    "    ab_df, ab_save = combine_AB(a, save_labels_path, b_laughter_paths)\n",
    "    ab_df.to_csv(ab_save, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_paths = [os.path.join(root, name)\n",
    "            for root, dirs, files in os.walk(save_labels_path)\n",
    "            for name in files\n",
    "            if name.endswith((\"Comb_Laugh.txt\"))]\n",
    "\n",
    "wavs = [os.path.join(root, name)\n",
    "            for root, dirs, files in os.walk(load_audio_path)\n",
    "            for name in files\n",
    "            if name.endswith((\".wav\"))]\n",
    "\n",
    "labels_paths_in_audio_format = [os.path.basename(l).split('_')[0] for l in label_paths]\n",
    "labels_paths_in_audio_format = [(l[0:2] + '0' + l[2:]) for l in labels_paths_in_audio_format]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create set of wavs which have a related label file\n",
    "wav_label_match = []\n",
    "for l in labels_paths_in_audio_format:\n",
    "    for w in wavs:\n",
    "        if l in w:\n",
    "            wav_label_match.append(w)\n",
    "wav_label_match = list(set(wav_label_match))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and save spectrograms\n",
    "for w in tqdm(wav_label_match, desc='total'):\n",
    "    y, sr = librosa.load(w, sr=sample_rate, mono=True)\n",
    "    length = int(len(y) / sr)\n",
    "    remainder = length % window_size\n",
    "    for i in tqdm(range(0, length - remainder - window_size, slide),\n",
    "                  desc='current_wav', leave=False):\n",
    "        S, S_save_path = create_spectrogram(w, save_spectros_path,\n",
    "                                            i, i + window_size,\n",
    "                                            y, sr, mels)\n",
    "        np.save(S_save_path, S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate out test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of conversation numbers in metadata format\n",
    "convs = [int(os.path.basename(p).split('.')[0][3:]) for p in wav_label_match]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_csv = 'swda-metadata.csv'\n",
    "df = pd.read_csv(metadata_csv)\n",
    "# filter dataframe to include only conversations with a wav file and a label\n",
    "#df = df[df['conversation_no'].isin(convs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['conversation_no',\n",
       " 'talk_day',\n",
       " 'length',\n",
       " 'topic_description',\n",
       " 'prompt',\n",
       " 'from_caller_sex',\n",
       " 'from_caller_education',\n",
       " 'from_caller_birth_year',\n",
       " 'from_caller_dialect_area',\n",
       " 'to_caller_sex',\n",
       " 'to_caller_education',\n",
       " 'to_caller_birth_year',\n",
       " 'to_caller_dialect_area']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are no ids, so need to create by combining other identifiers\n",
    "df['from_id'] = df['from_caller_sex'].astype(str) + \",\" \\\n",
    "                + df['from_caller_birth_year'].astype(str) + \",\" \\\n",
    "                + df['from_caller_education'].astype(str) + \",\" \\\n",
    "                + df['from_caller_dialect_area'].astype(str)\n",
    "df['to_id'] = df['to_caller_sex'].astype(str) + \",\" \\\n",
    "                + df['to_caller_birth_year'].astype(str) + \",\" \\\n",
    "                + df['to_caller_education'].astype(str) + \",\" \\\n",
    "                + df['to_caller_dialect_area'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['talk_day', 'length',\n",
    "                      'topic_description', 'prompt',\n",
    "                      'from_caller_sex', 'from_caller_education',\n",
    "                      'from_caller_birth_year', 'from_caller_dialect_area',\n",
    "                      'to_caller_sex', 'to_caller_education',\n",
    "                      'to_caller_birth_year', 'to_caller_dialect_area'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create graph \n",
    "edges = df.drop(columns=['conversation_no'])\n",
    "G = nx.convert_matrix.from_pandas_edgelist(edges, source='from_id', target='to_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of conversations with lowest degree centrality nodes to form test set\n",
    "central_dict = nx.degree_centrality(G)\n",
    "ids = [k for k in central_dict.keys() if central_dict[k] < 0.01]\n",
    "f_id_df = df[df['from_id'].isin(ids)]\n",
    "t_id_df = df[df['to_id'].isin(ids)]\n",
    "test_set = pd.concat([f_id_df,t_id_df]).drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_no</th>\n",
       "      <th>from_id</th>\n",
       "      <th>to_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2121</td>\n",
       "      <td>MALE,1937,2,NEW ENGLAND</td>\n",
       "      <td>MALE,1958,3,NORTHERN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2131</td>\n",
       "      <td>MALE,1933,2,SOUTH MIDLAND</td>\n",
       "      <td>FEMALE,1963,2,SOUTH MIDLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2151</td>\n",
       "      <td>MALE,1932,1,NEW ENGLAND</td>\n",
       "      <td>FEMALE,1963,2,SOUTH MIDLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2229</td>\n",
       "      <td>MALE,1956,2,NORTHERN</td>\n",
       "      <td>FEMALE,1957,2,MIXED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2429</td>\n",
       "      <td>FEMALE,1970,2,SOUTHERN</td>\n",
       "      <td>FEMALE,1959,2,MIXED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>4905</td>\n",
       "      <td>FEMALE,1960,2,WESTERN</td>\n",
       "      <td>MALE,1958,3,SOUTHERN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>4908</td>\n",
       "      <td>MALE,1937,2,NORTHERN</td>\n",
       "      <td>MALE,1945,3,NORTH MIDLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>4917</td>\n",
       "      <td>FEMALE,1960,2,WESTERN</td>\n",
       "      <td>MALE,1962,2,NORTHERN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>4927</td>\n",
       "      <td>FEMALE,1960,2,WESTERN</td>\n",
       "      <td>MALE,1946,1,WESTERN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>4928</td>\n",
       "      <td>MALE,1937,2,NORTHERN</td>\n",
       "      <td>MALE,1969,2,NORTH MIDLAND</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     conversation_no                    from_id                        to_id\n",
       "0               2121    MALE,1937,2,NEW ENGLAND         MALE,1958,3,NORTHERN\n",
       "1               2131  MALE,1933,2,SOUTH MIDLAND  FEMALE,1963,2,SOUTH MIDLAND\n",
       "2               2151    MALE,1932,1,NEW ENGLAND  FEMALE,1963,2,SOUTH MIDLAND\n",
       "3               2229       MALE,1956,2,NORTHERN          FEMALE,1957,2,MIXED\n",
       "4               2429     FEMALE,1970,2,SOUTHERN          FEMALE,1959,2,MIXED\n",
       "..               ...                        ...                          ...\n",
       "187             4905      FEMALE,1960,2,WESTERN         MALE,1958,3,SOUTHERN\n",
       "188             4908       MALE,1937,2,NORTHERN    MALE,1945,3,NORTH MIDLAND\n",
       "189             4917      FEMALE,1960,2,WESTERN         MALE,1962,2,NORTHERN\n",
       "190             4927      FEMALE,1960,2,WESTERN          MALE,1946,1,WESTERN\n",
       "191             4928       MALE,1937,2,NORTHERN    MALE,1969,2,NORTH MIDLAND\n",
       "\n",
       "[192 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of all conversation nos in test set\n",
    "test_set_list = [str(e) for e in test_set['conversation_no'].tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset (combine, id, spectrogram, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_label_start_end(spectro_file, save_labels_path):\n",
    "    \"\"\"Find the label path, start and end time relating to spectrogram.\n",
    "    \n",
    "    # Arguments\n",
    "        spectro_file: spectrogram filepath.\n",
    "        save_labels_path: directory of labels.\n",
    "        \n",
    "    # Returns\n",
    "        label_path: label filepath relating to the spectrogram.\n",
    "        start_time: start time relating to the spectrogram.\n",
    "        end_time: end time relating to the spectrogram.\n",
    "    \"\"\"\n",
    "    base_file = os.path.basename(spectro_file)\n",
    "    time = base_file.split('_')[1]\n",
    "    start_time = int(time.split('to')[0])\n",
    "    end_time = int(time.split('to')[1])\n",
    "\n",
    "    spec_id = os.path.basename(spectro_file.split('_')[0])[3:]\n",
    "    label_dir = os.path.dirname(spectro_file)\n",
    "    label_files = [f for f in os.listdir(save_labels_path) \n",
    "                   if f.endswith((spec_id + \"_Comb_Laugh.txt\"))]\n",
    "    label_path = save_labels_path + '/' + label_files[0]\n",
    "    return label_path, start_time, end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_csv(start_time, end_time, label_path):\n",
    "    \"\"\"Filters csv file for start and end time, returns as dataframe.\n",
    "    \n",
    "    # Arguments\n",
    "        start_time: start time relating to spectrogram.\n",
    "        end_time: end time relating to spectrogram.\n",
    "        label_path: filepath of label.\n",
    "        \n",
    "    # Returns\n",
    "        dataframe filtered to contain 'laugh' in the text\n",
    "        and filtered for specified start_time and end_time.\n",
    "        When start_time in the csv is before specified start_time,\n",
    "        this record will be included but start_time in the csv will be set\n",
    "        to specified start_time. Same for end_time.\n",
    "        \n",
    "    # Example\n",
    "        start         end          label1\n",
    "        905.765658    909.731864   [laughter]\n",
    "\n",
    "        if start_time was 907 and end_time was 909, this row would be set to:\n",
    "\n",
    "        start         end          label1\n",
    "        907.0         909.0        [laughter]\n",
    "    \"\"\"    \n",
    "    df = pd.read_csv(label_path, sep='\\t', index_col=0)\n",
    "    df = df[df['start'] <= end_time]\n",
    "    df = df[df['end'] >= start_time]\n",
    "    df.loc[df.end > end_time, 'end'] = end_time\n",
    "    df.loc[df.start < start_time, 'start'] = start_time\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_id(spectro_file):\n",
    "    \"\"\"Create identifier for spectrogram.\n",
    "    \n",
    "    # Arguments\n",
    "        spectro_file: filepath for spectrogram.\n",
    "        \n",
    "    # Returns\n",
    "        id for file.\n",
    "        For example input of spectro_file of '.../sw03148/sw03148_264to270_spectro.npy'\n",
    "        would return 'sw031_270to276'.\n",
    "    \"\"\"\n",
    "    base_name = os.path.basename(spectro_file)\n",
    "    r = base_name.split('_')[0]\n",
    "    r2 = base_name.split('_')[1]\n",
    "    file_id = r + '_' + r2\n",
    "    file_id\n",
    "    return file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_end_in_timesteps(df, start_time, timesteps_per_second):\n",
    "    \"\"\"Convert start and end time from seconds to timesteps.\n",
    "    Remove tier_name, tier_type and text columns.\n",
    "    Reformat times to start from 0 and end at window_size.\n",
    "    \n",
    "    # Arguments\n",
    "        df: dataframe in format from output of function filter_csv.\n",
    "        start_time: start time relating to spectrogram.\n",
    "        timesteps_per_second: timesteps_per_second = timesteps / window_size.\n",
    "        \n",
    "    # Returns\n",
    "        dataframe after modifications.\n",
    "    \"\"\"\n",
    "    df = df.drop(['id', 'label1', 'label2'], 1)\n",
    "    df['start'] = df['start'] - start_time\n",
    "    df['start'] = (df['start'] * timesteps_per_second).apply(np.floor)\n",
    "    df['end'] = df['end'] - start_time\n",
    "    df['end'] = (df['end'] * timesteps_per_second).apply(np.ceil)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label_matrix(df):\n",
    "    \"\"\"Convert label annotations into a matrix.\n",
    "    \n",
    "    # Arguments\n",
    "        df: dataframe in format from output of start_end_in_timesteps.\n",
    "        \n",
    "    # Returns\n",
    "        vector of length (timesteps) which has values of 0 or 1.\n",
    "        1 representing laughter, 0 representing no laughter.\n",
    "    \n",
    "    # Example\n",
    "        [1, 0, 0, 1, 0, 0 ....] represents laughter in timesteps 0 and 3\n",
    "    \"\"\"\n",
    "    label = np.zeros(timesteps)\n",
    "    update_list = []\n",
    "    for index, row in df.iterrows():\n",
    "        update_list.append([row['start'], row['end']])\n",
    "    for l in update_list:\n",
    "        start = int(l[0])\n",
    "        end = int(l[1])\n",
    "        label[start:end] = 1\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_id_spectro_label(file_id, spectro_path, label):\n",
    "    \"\"\"Combine id, spectrogram and label.\n",
    "    \n",
    "    # Arguments\n",
    "        file_id: file id created from function create_id.\n",
    "        spectro_path: filepath for spectrogram.\n",
    "        label: label created from function create_label_matrix.\n",
    "        \n",
    "    # Returns\n",
    "        numpy array containing 3 elements:\n",
    "        id\n",
    "        related spectrogram\n",
    "        related label\n",
    "    \"\"\"\n",
    "    np_spectro_file = np.load(spectro_path)\n",
    "    combined = [file_id, np_spectro_file, label]\n",
    "    np_combined = np.asarray(combined)\n",
    "    return np_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectros = [os.path.join(root, name)\n",
    "            for root, dirs, files in os.walk(save_spectros_path)\n",
    "            for name in files\n",
    "            if name.endswith((\"spectro.npy\"))]\n",
    "\n",
    "# keep only spectrograms that have a label\n",
    "spectros_keep = []\n",
    "# find conversations that match with wavs\n",
    "conv_wav_match = [os.path.basename(x).split('.')[0][3:] for x in wav_label_match]\n",
    "for s in spectros:\n",
    "    conv_s = os.path.dirname(s).split('/')[-1][3:]\n",
    "    if conv_s in conv_wav_match:\n",
    "        spectros_keep.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for spectro_path in tqdm(spectros_keep, desc='create dataset'):\n",
    "    label_path, start_time, end_time = find_label_start_end(spectro_path, save_labels_path)\n",
    "    df = filter_csv(start_time, end_time, label_path)\n",
    "    df = start_end_in_timesteps(df, start_time, timesteps_per_second)\n",
    "    df_label = create_label_matrix(df)\n",
    "    file_id = create_id(spectro_path)\n",
    "    np_combined = create_id_spectro_label(file_id, spectro_path, df_label)\n",
    "    dataset.append(np_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check laughter rate and save train, val, test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increase_laugh_rate(dataset, percentage_laughs):\n",
    "    \"\"\"Increase the laughter rate in the dataset.\n",
    "    \n",
    "    # Arguments\n",
    "        dataset: dataset to have laughter percentage increased.\n",
    "        percentage_laughs: desired percentage for dataset of\n",
    "                           examples containing a laugh. As integer.\n",
    "    \n",
    "    # Returns\n",
    "        dataset_inc_laughs: dataset with increased rate of examples \n",
    "                            containing a laugh.\n",
    "    \"\"\"\n",
    "    laugh = [example for example in dataset if 1 in example[2]]\n",
    "    laugh_count = len(laugh)\n",
    "    no_laugh = [example for example in dataset if 1 not in example[2]]\n",
    "    no_laugh_count = len(no_laugh)\n",
    "    \n",
    "    delete_from_no_laugh = len(dataset) - laugh_count * 100 / percentage_laughs\n",
    "    delete_from_no_laugh = int(delete_from_no_laugh)\n",
    "    \n",
    "    no_laugh = no_laugh[0:-delete_from_no_laugh]\n",
    "    \n",
    "    dataset_inc_laughs = np.vstack((laugh, no_laugh))\n",
    "    return dataset_inc_laughs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laughter_check(dataset):\n",
    "    \"\"\"Check the percentage of clips that contain laughter in the dataset.\n",
    "    \n",
    "    # Arguments\n",
    "        dataset: dataset to be checked.\n",
    "    \n",
    "    # Returns\n",
    "        percentage of examples containing laughter.\n",
    "    \"\"\"\n",
    "    laughs = len([e for e in dataset[:, 2] if 1 in e])\n",
    "    total = len(dataset[:, 2])\n",
    "    laughter_p = laughs / total\n",
    "    return laughter_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.asarray(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val = [e for e in dataset \n",
    "             if e[0].split('_')[0][3:] not in test_set_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [e for e in dataset \n",
    "        if e[0].split('_')[0][3:] in test_set_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15669655497876356\n",
      "0.16479772888573457\n",
      "0.12847288752634603\n"
     ]
    }
   ],
   "source": [
    "train_val = np.asarray(train_val)\n",
    "np.random.shuffle(train_val)\n",
    "val_split = int(len(dataset) * 0.1)\n",
    "train = train_val[:-val_split]\n",
    "val = train_val[-val_split:]\n",
    "\n",
    "test = np.asarray(test)\n",
    "np.random.shuffle(test)\n",
    "print(laughter_check(train))\n",
    "print(laughter_check(val))\n",
    "print(laughter_check(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = increase_laugh_rate(train, 33)\n",
    "print(laughter_check(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(save_dataset_path + 'en_train_6_6_64_ds', train)\n",
    "np.save(save_dataset_path + 'en_val_6_6_64_ds', val)\n",
    "np.save(save_dataset_path + 'en_test_6_6_64_ds', test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
